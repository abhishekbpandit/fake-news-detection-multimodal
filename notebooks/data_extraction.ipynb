{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eabbc2cb-1436-41b2-a97f-2032a4820cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import threading\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd5d8bf-b01f-4d3e-8e4f-a35235a5945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_drive(file_id, destination_file_name):\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        'credentials.json',\n",
    "        scopes=['https://www.googleapis.com/auth/drive.readonly']\n",
    "    )\n",
    "\n",
    "    drive_service = build('drive', 'v3', credentials=credentials)\n",
    "\n",
    "                          \n",
    "    request = drive_service.files().get_media(fileId=file_id)\n",
    "    file_stream = io.FileIO(destination_file_name, 'wb')\n",
    "    downloader = MediaIoBaseDownload(file_stream, request)\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        status, done = downloader.next_chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae7fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket_name, local_file, destination_file):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    blob = bucket.blob(destination_file)\n",
    "    blob.upload_from_filename(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa778837-02b5-4e2e-a283-95890c449b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_upload_tar_bz2(source_file, destination_folder, bucket_name):\n",
    "    # If the destination folder exists, remove it\n",
    "    if os.path.exists(destination_folder):\n",
    "        shutil.rmtree(destination_folder)\n",
    "\n",
    "    # Create the destination folder\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "    # Create a queue to hold files to be uploaded\n",
    "    upload_queue = Queue(maxsize=10)\n",
    "\n",
    "    # Create a ThreadPoolExecutor for uploading\n",
    "    with ThreadPoolExecutor(max_workers=5) as upload_executor:\n",
    "        # Start a separate thread for extraction\n",
    "        def extract_files():\n",
    "            with tarfile.open(source_file, 'r:bz2') as tar:\n",
    "                for member in tar:\n",
    "                    if member.isfile():\n",
    "                        # extract member (file) with its full hierarchical name\n",
    "                        tar.extract(member, path=destination_folder)\n",
    "                        local_file = os.path.join(destination_folder, member.name)\n",
    "\n",
    "                        # Put the file in the queue for uploading\n",
    "                        upload_queue.put((local_file, \"images/\" + member.name))\n",
    "\n",
    "            # Put a sentinel in the queue to signal the end of extraction\n",
    "            upload_queue.put(None)\n",
    "\n",
    "        extract_thread = threading.Thread(target=extract_files)\n",
    "        extract_thread.start()\n",
    "\n",
    "        # Function for workers to upload files\n",
    "        def upload_worker():\n",
    "            while True:\n",
    "                job = upload_queue.get()\n",
    "                if job is None:\n",
    "                    # If the job is the sentinel, end the worker\n",
    "                    break\n",
    "                local_file, destination_file = job\n",
    "                upload_to_gcs(bucket_name, local_file, destination_file)\n",
    "                os.remove(local_file)\n",
    "\n",
    "        # Start the upload workers\n",
    "        for _ in range(5):\n",
    "            upload_executor.submit(upload_worker)\n",
    "\n",
    "        # Wait for extraction to finish, then put sentinels in the queue to end the upload workers\n",
    "        extract_thread.join()\n",
    "        for _ in range(5):\n",
    "            upload_queue.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e2df12-c01a-4c3b-b3bf-d927a46d3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the Google Drive file ID and destination file name for download\n",
    "file_id = '1cjY6HsHaSZuLVHywIxD5xQqng33J5S2b'\n",
    "destination_file_name = 'downloaded_data.tar.bz2'\n",
    "\n",
    "# Provide the GCS bucket name and destination folder path\n",
    "bucket_name = 'fake-news-data'\n",
    "destination_folder = 'fakeddit'\n",
    "\n",
    "# Specify the folder path where the extracted files will be saved\n",
    "extracted_folder = 'extracted_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146d028-6282-480f-a852-6d0376680b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_from_drive(file_id, destination_file_name)\n",
    "extract_and_upload_tar_bz2(destination_file_name, extracted_folder, bucket_name)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
